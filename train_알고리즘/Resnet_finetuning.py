import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
from tqdm import tqdm
from torchvision.models import ResNet50_Weights, resnet50
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, f1_score
import json

# ==========================================
# 1. ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ==========================================
train_dir = "./data/train"  # í•™ìŠµ ë°ì´í„° ê²½ë¡œ
test_dir = "./data/test"    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ

# [ë³€ê²½] GPU 4ê°œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì“°ê¸° ìœ„í•´ ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ëŠ˜ë¦¼ (32 -> 128)
# ë§Œì•½ "CUDA out of memory" ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ 64 ë˜ëŠ” 32ë¡œ ì¤„ì´ì„¸ìš”.
BATCH_SIZE = 128 
NUM_WORKERS = 16  # ë°ì´í„° ë¡œë”© ì†ë„ í–¥ìƒ

# ==========================================
# 2. ë°ì´í„° ì „ì²˜ë¦¬
# ==========================================
transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225],
    ),
])

transform_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225],
    ),
])

# ==========================================
# 3. ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë”
# ==========================================
if not os.path.exists(train_dir) or not os.path.exists(test_dir):
    print("ë°ì´í„° ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    # exit()

train_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)
test_dataset = datasets.ImageFolder(root=test_dir, transform=transform_test)

# [ë³€ê²½] num_workersì™€ batch_size ì ìš©
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

print(f"í›ˆë ¨ì…‹ í¬ê¸°: {len(train_dataset)}")
print(f"í…ŒìŠ¤íŠ¸ì…‹ í¬ê¸°: {len(test_dataset)}")

num_classes = len(train_dataset.classes)

# ==========================================
# 4. ëª¨ë¸ ì„¤ì • ë° GPU ë³‘ë ¬í™”
# ==========================================
# ì‚¬ì „ í•™ìŠµëœ ResNet50 ë¶ˆëŸ¬ì˜¤ê¸°
model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)

# ëª¨ë“  íŒŒë¼ë¯¸í„° freeze (ë§ˆì§€ë§‰ fc ì œì™¸)
for param in model.parameters():
    param.requires_grad = False

# ìµœìƒìœ„ ë ˆì´ì–´ ë³€ê²½ (ëœë¤ ì´ˆê¸°í™”)
model.fc = nn.Linear(model.fc.in_features, num_classes)

# GPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# [í•µì‹¬ ë³€ê²½] Multi-GPU ì„¤ì • (DataParallel)
if torch.cuda.device_count() > 1:
    print(f"ğŸ”¥ [System] ê°ì§€ëœ GPU ê°œìˆ˜: {torch.cuda.device_count()}ê°œ")
    print("ğŸ”¥ DataParallelì„ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤!")
    model = nn.DataParallel(model)
else:
    print("CUDA is not available or only 1 GPU found. Using single device.")

model = model.to(device)

# ì†ì‹¤ í•¨ìˆ˜
criterion = nn.CrossEntropyLoss()

# ==========================================
# 5. í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
# ==========================================
def train_and_test(
    model,
    train_loader,
    test_loader,
    criterion,
    num_epochs=30,
    log_file="resnet_training_log.txt",
):
    best_test_acc = 0.0  # ìµœê³  í…ŒìŠ¤íŠ¸ ì •í™•ë„ ì¶”ì 

    # ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™” ë° í—¤ë” ì‘ì„±
    with open(log_file, mode="w") as file:
        file.write("Epoch\tTrain Loss\tTrain Acc\tTest Loss\tTest Acc\tTest F1\n")

    # [ì¤‘ìš”] DataParallel ì‚¬ìš© ì‹œ, ì‹¤ì œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ì ‘ê·¼í•˜ë ¤ë©´ .moduleì„ ì‚¬ìš©í•´ì•¼ í•¨
    # ëª¨ë¸ì´ DataParallelë¡œ ê°ì‹¸ì ¸ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì‹¤ì œ ëª¨ë¸ ê°ì²´(real_model)ë¥¼ ì°¸ì¡°
    real_model = model.module if hasattr(model, 'module') else model
    
    # ì´ˆê¸° Optimizer ì„¤ì • (ì„ì‹œ)
    optimizer = optim.Adam(real_model.fc.parameters(), lr=1e-4)

    for epoch in range(num_epochs):
        print(f"\nEpoch {epoch+1}/{num_epochs}")
        print("-" * 30)

        # ---------------------------------------
        # ë™ì  Learning Rate ë° Unfreezing ì „ëµ
        # ---------------------------------------
        if epoch == 0:
            print("ì²« ë²ˆì§¸ epoch: ë§ˆì§€ë§‰ ì¶œë ¥ì¸µë§Œ í•™ìŠµ")
            # ì „ì²´ ë™ê²°
            for param in real_model.parameters(): 
                param.requires_grad = False
            # FC ë ˆì´ì–´ë§Œ í•´ì œ
            for param in real_model.fc.parameters(): 
                param.requires_grad = True
            
            optimizer = optim.Adam(real_model.fc.parameters(), lr=1e-4)
            
        elif epoch == 1:
            print("ë‘ ë²ˆì§¸ epochë¶€í„° ì „ì²´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì • (Fine-tuning)")
            # ì „ì²´ í•´ì œ
            for param in real_model.parameters():
                param.requires_grad = True
            
            optimizer = optim.Adam(real_model.parameters(), lr=1e-5)

        # ---------------------------------------
        # í•™ìŠµ ë‹¨ê³„ (Training)
        # ---------------------------------------
        model.train()  # í•™ìŠµ ëª¨ë“œ
        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in tqdm(train_loader, desc="Training"):
            # ë°ì´í„°ë¥¼ CUDA ì¥ì¹˜ë¡œ ì´ë™
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            _, preds = torch.max(outputs, 1)

            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

        epoch_train_loss = running_loss / len(train_loader.dataset)
        epoch_train_acc = running_corrects.double() / len(train_loader.dataset)
        print(f"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f}")

        # ---------------------------------------
        # í‰ê°€ ë‹¨ê³„ (Testing)
        # ---------------------------------------
        model.eval()  # í‰ê°€ ëª¨ë“œ
        test_loss = 0.0
        test_corrects = 0
        all_test_preds = []
        all_test_labels = []

        with torch.no_grad():
            for inputs, labels in tqdm(test_loader, desc="Testing"):
                inputs = inputs.to(device)
                labels = labels.to(device)

                outputs = model(inputs)
                loss = criterion(outputs, labels)
                _, preds = torch.max(outputs, 1)

                test_loss += loss.item() * inputs.size(0)
                test_corrects += torch.sum(preds == labels.data)
                
                # CPUë¡œ ì´ë™í•˜ì—¬ ê²°ê³¼ ëª¨ìœ¼ê¸°
                all_test_preds.extend(preds.cpu().numpy())
                all_test_labels.extend(labels.cpu().numpy())

        epoch_test_loss = test_loss / len(test_loader.dataset)
        epoch_test_acc = test_corrects.double() / len(test_loader.dataset)
        epoch_test_f1 = f1_score(all_test_labels, all_test_preds, average="macro")
        
        print(
            f"Test Loss: {epoch_test_loss:.4f} | Test Acc: {epoch_test_acc:.4f} | Test F1: {epoch_test_f1:.4f}"
        )

        # ---------------------------------------
        # ë¡œê·¸ ë° ëª¨ë¸ ì €ì¥
        # ---------------------------------------
        with open(log_file, mode="a") as file:
            file.write(
                f"{epoch+1}\t{epoch_train_loss:.4f}\t{epoch_train_acc:.4f}\t{epoch_test_loss:.4f}\t{epoch_test_acc:.4f}\t{epoch_test_f1:.4f}\n"
            )

        if epoch_test_acc > best_test_acc:
            best_test_acc = epoch_test_acc
            # [ì¤‘ìš”] ì €ì¥í•  ë•ŒëŠ” DataParallel ê»ë°ê¸°ë¥¼ ë²—ê¸°ê³ (real_model) ì €ì¥í•´ì•¼ ë‚˜ì¤‘ì— ë¶ˆëŸ¬ì˜¤ê¸° í¸í•¨
            torch.save(real_model.state_dict(), "best_finetuned_resnet.pth")
            print("Best model saved.")

    print(f"\nTraining complete. Best Test Acc: {best_test_acc:.4f}")

    # ==========================================
    # 6. ìµœì¢… ë¶„ì„ ê²°ê³¼ ì €ì¥
    # ==========================================
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc="Final Evaluation"):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)

    # Classification Report ì €ì¥
    class_names = train_loader.dataset.classes
    report = classification_report(
        all_labels, all_preds, target_names=class_names, output_dict=True
    )

    with open("resnet_classification_report.json", "w") as f:
        json.dump(report, f, indent=4)

    print("\nClassification Report saved to resnet_classification_report.json")

    # Confusion Matrix ì €ì¥
    cm = confusion_matrix(all_labels, all_preds)
    np.save("resnet_confusion_matrix.npy", cm)
    print("Confusion Matrix saved to resnet_confusion_matrix.npy")

    # ì˜ˆì¸¡ í™•ë¥  ì €ì¥ (ROC curve ë¶„ì„ìš©)
    np.save("resnet_test_predictions.npy", all_preds)
    np.save("resnet_test_labels.npy", all_labels)
    np.save("resnet_test_probs.npy", all_probs)
    print("Test predictions and probabilities saved.")

    return model


if __name__ == "__main__":
    # ëª¨ë¸ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    model = train_and_test(
        model,
        train_loader,
        test_loader,
        criterion,
        num_epochs=30,
        log_file="resnet_training_log.txt",
    )