import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, f1_score
import json
import timm  # timm ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏÇ¨Ïö© (pip install timm)

# ==========================================
# 1. ÏÑ§Ï†ï Î∞è ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞
# ==========================================
train_dir = "./data/train"  # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú
test_dir = "./data/test"    # ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú

# [Î≥ÄÍ≤Ω] GPU 4Í∞úÎ•º Ìö®Ïú®Ï†ÅÏúºÎ°ú Ïì∞Í∏∞ ÏúÑÌï¥ Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à Ï¶ùÍ∞Ä (32 -> 128)
# Î©îÎ™®Î¶¨ ÏóêÎü¨ Î∞úÏÉù Ïãú 64 ÎòêÎäî 32Î°ú Ï°∞Ï†ïÌïòÏÑ∏Ïöî.
BATCH_SIZE = 128
NUM_WORKERS = 16  # Îç∞Ïù¥ÌÑ∞ Î°úÎî© ÏÜçÎèÑ Ìñ•ÏÉÅ

# Î°úÍ∑∏ Î∞è Í≤∞Í≥º ÌååÏùº Í≤ΩÎ°ú ÏÑ§Ï†ï (ResNetÍ≥º Í≤πÏπòÏßÄ ÏïäÍ≤å ÏÑ§Ï†ï)
LOG_FILE = "efficientnet_training_log.txt"
MODEL_SAVE_NAME = "best_finetuned_efficientnet.pth"
REPORT_SAVE_NAME = "efficientnet_classification_report.json"
CM_SAVE_NAME = "efficientnet_confusion_matrix.npy"

# ==========================================
# 2. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨
# ==========================================
# EfficientNetÏùÄ 224x224 (B0 Í∏∞Ï§Ä) ÏûÖÎ†• ÏÇ¨Ïö©
transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225],
    ),
])

transform_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225],
    ),
])

# ==========================================
# 3. Îç∞Ïù¥ÌÑ∞ÏÖã Î∞è Îç∞Ïù¥ÌÑ∞Î°úÎçî
# ==========================================
if not os.path.exists(train_dir) or not os.path.exists(test_dir):
    print("Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°úÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.")
    # exit()

train_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)
test_dataset = datasets.ImageFolder(root=test_dir, transform=transform_test)

# [Î≥ÄÍ≤Ω] ÏÉÅÌñ•Îêú Batch SizeÏôÄ Num Workers Ï†ÅÏö©
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

print(f"ÌõàÎ†®ÏÖã ÌÅ¨Í∏∞: {len(train_dataset)}")
print(f"ÌÖåÏä§Ìä∏ÏÖã ÌÅ¨Í∏∞: {len(test_dataset)}")
print(f"ÌÅ¥ÎûòÏä§ Ïàò: {len(train_dataset.classes)}")

# ==========================================
# 4. Î™®Îç∏ ÏÑ§Ï†ï Î∞è GPU Î≥ëÎ†¨Ìôî
# ==========================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# EfficientNet-B0 Î™®Îç∏ ÏÉùÏÑ± (pretrained=True)
model = timm.create_model(
    "efficientnet_b0", pretrained=True, num_classes=len(train_dataset.classes)
)

print(f"\nEfficientNet-B0 Î™®Îç∏ Î°úÎìú ÏôÑÎ£å.")

# [ÌïµÏã¨ Î≥ÄÍ≤Ω] Multi-GPU ÏÑ§Ï†ï (DataParallel)
if torch.cuda.device_count() > 1:
    print(f"üî• [System] Í∞êÏßÄÎêú GPU Í∞úÏàò: {torch.cuda.device_count()}Í∞ú")
    print("üî• DataParallelÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î≥ëÎ†¨ ÌïôÏäµÏùÑ ÏãúÏûëÌï©ÎãàÎã§!")
    model = nn.DataParallel(model)
else:
    print(f"Using single device: {device}")

model = model.to(device)

# ÏÜêÏã§ Ìï®Ïàò
criterion = nn.CrossEntropyLoss()

# ==========================================
# 5. ÌïôÏäµ Î∞è ÌÖåÏä§Ìä∏ Ìï®Ïàò
# ==========================================
def train_and_test(
    model,
    train_loader,
    test_loader,
    criterion,
    num_epochs=30,
    log_file=LOG_FILE,
):
    best_test_acc = 0.0

    # Î°úÍ∑∏ Ìè¥Îçî ÏÉùÏÑ± Î∞è Ìó§Îçî ÏûëÏÑ±
    os.makedirs("./log", exist_ok=True)
    log_path = f"./log/{log_file}"

    with open(log_path, mode="w") as file:
        file.write("Epoch\tTrain Loss\tTrain Acc\tTest Loss\tTest Acc\tTest F1\n")

    print("\n" + "=" * 70)
    print("EfficientNet-B0 ÌïôÏäµ ÏãúÏûë")
    print("=" * 70)

    # [Ï§ëÏöî] DataParallel ÏÇ¨Ïö© Ïãú, ÎÇ¥Î∂Ä ÌååÎùºÎØ∏ÌÑ∞ Ï†úÏñ¥Î•º ÏúÑÌï¥ .moduleÎ°ú Ï†ëÍ∑º
    real_model = model.module if hasattr(model, 'module') else model

    # Ï¥àÍ∏∞ ÏòµÌã∞ÎßàÏù¥Ï†Ä (ÏûÑÏãú)
    optimizer = optim.Adam(real_model.parameters(), lr=1e-3)

    for epoch in range(num_epochs):
        print(f"\n[Epoch {epoch+1}/{num_epochs}]")
        
        # ---------------------------------------
        # ÎèôÏ†Å Learning Rate Î∞è Layer Freezing
        # ---------------------------------------
        if epoch == 0:
            # Ï≤´ Î≤àÏß∏ epoch: classifierÎßå ÌïôÏäµ
            print("Ï≤´ Î≤àÏß∏ epoch: ClassifierÎßå ÌïôÏäµ (Feature Extraction)")
            for name, param in real_model.named_parameters():
                if "classifier" in name: # timm EfficientNetÏùò ÎßàÏßÄÎßâ Ï∏µ Ïù¥Î¶ÑÏùÄ 'classifier'
                    param.requires_grad = True
                else:
                    param.requires_grad = False
            
            optimizer = optim.Adam(
                filter(lambda p: p.requires_grad, real_model.parameters()), lr=1e-3
            )
            
        elif epoch == 1:
            # Îëê Î≤àÏß∏ epochÎ∂ÄÌÑ∞ Ï†ÑÏ≤¥ Î™®Îç∏ ÎØ∏ÏÑ∏ Ï°∞Ï†ï
            print("Îëê Î≤àÏß∏ epochÎ∂ÄÌÑ∞ Ï†ÑÏ≤¥ Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ï (Fine-tuning)")
            for param in real_model.parameters():
                param.requires_grad = True
            
            optimizer = optim.Adam(real_model.parameters(), lr=1e-4)

        # ---------------------------------------
        # ÌïôÏäµ Îã®Í≥Ñ (Training)
        # ---------------------------------------
        model.train()
        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in tqdm(train_loader, desc="Training"):
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            _, preds = torch.max(outputs, 1)

            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

        epoch_train_loss = running_loss / len(train_loader.dataset)
        epoch_train_acc = running_corrects.double() / len(train_loader.dataset)
        print(f"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f}")

        # ---------------------------------------
        # ÌèâÍ∞Ä Îã®Í≥Ñ (Testing)
        # ---------------------------------------
        model.eval()
        test_loss = 0.0
        test_corrects = 0
        all_test_preds = []
        all_test_labels = []

        with torch.no_grad():
            for inputs, labels in tqdm(test_loader, desc="Testing"):
                inputs = inputs.to(device)
                labels = labels.to(device)

                outputs = model(inputs)
                loss = criterion(outputs, labels)
                _, preds = torch.max(outputs, 1)

                test_loss += loss.item() * inputs.size(0)
                test_corrects += torch.sum(preds == labels.data)
                
                # CPUÎ°ú Ïù¥ÎèôÌïòÏó¨ Í≤∞Í≥º ÏàòÏßë
                all_test_preds.extend(preds.cpu().numpy())
                all_test_labels.extend(labels.cpu().numpy())

        epoch_test_loss = test_loss / len(test_loader.dataset)
        epoch_test_acc = test_corrects.double() / len(test_loader.dataset)
        epoch_test_f1 = f1_score(all_test_labels, all_test_preds, average="macro")
        
        print(
            f"Test Loss: {epoch_test_loss:.4f} | Test Acc: {epoch_test_acc:.4f} | Test F1: {epoch_test_f1:.4f}"
        )

        # Î°úÍ∑∏ Ï†ÄÏû•
        with open(log_path, mode="a") as file:
            file.write(
                f"{epoch+1}\t{epoch_train_loss:.4f}\t{epoch_train_acc:.4f}\t{epoch_test_loss:.4f}\t{epoch_test_acc:.4f}\t{epoch_test_f1:.4f}\n"
            )

        # ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏ Ï†ÄÏû• (real_model Ï†ÄÏû•)
        if epoch_test_acc > best_test_acc:
            best_test_acc = epoch_test_acc
            torch.save(real_model.state_dict(), MODEL_SAVE_NAME)
            print(f"‚úì Best model saved to {MODEL_SAVE_NAME}")

    print("\n" + "=" * 70)
    print(f"ÌïôÏäµ ÏôÑÎ£å! Best Test Acc: {best_test_acc:.4f}")
    print("=" * 70)

    # ==========================================
    # 6. ÏµúÏ¢Ö Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû•
    # ==========================================
    print("\nÏµúÏ¢Ö ÌèâÍ∞Ä Î∞è Î©îÌä∏Î¶≠ Ï†ÄÏû• Ï§ë...")
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc="Final Evaluation"):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)

    # 1. Classification Report (JSON)
    class_names = train_loader.dataset.classes
    report = classification_report(
        all_labels, all_preds, target_names=class_names, output_dict=True
    )
    with open(REPORT_SAVE_NAME, "w") as f:
        json.dump(report, f, indent=4)
    print(f"‚úì Classification Report saved to {REPORT_SAVE_NAME}")

    # 2. Confusion Matrix (NPY)
    cm = confusion_matrix(all_labels, all_preds)
    np.save(CM_SAVE_NAME, cm)
    print(f"‚úì Confusion Matrix saved to {CM_SAVE_NAME}")

    # 3. Probabilities (NPY) for ROC/Ensemble
    np.save("efficientnet_test_predictions.npy", all_preds)
    np.save("efficientnet_test_labels.npy", all_labels)
    np.save("efficientnet_test_probs.npy", all_probs)
    print("‚úì Test predictions and probabilities saved (efficientnet_ prefix).")

    return model


if __name__ == "__main__":
    model = train_and_test(
        model,
        train_loader,
        test_loader,
        criterion,
        num_epochs=30,
        log_file=LOG_FILE,
    )