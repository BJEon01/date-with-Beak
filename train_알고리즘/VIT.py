import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
from tqdm import tqdm
from torchvision.models import ViT_B_16_Weights  # ViT Î™®Îç∏Ïùò Í∞ÄÏ§ëÏπò Í∞ÄÏ†∏Ïò§Í∏∞
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, f1_score
import json

# ==========================================
# 1. ÏÑ§Ï†ï Î∞è ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞
# ==========================================
train_dir = "./data/train"  # ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú
test_dir = "./data/test"    # ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°ú

# [Î≥ÄÍ≤Ω] GPU 4Í∞úÎ•º Ìö®Ïú®Ï†ÅÏúºÎ°ú Ïì∞Í∏∞ ÏúÑÌï¥ Î∞∞Ïπò ÏÇ¨Ïù¥Ï¶à Ï¶ùÍ∞Ä (32 -> 128)
# Î©îÎ™®Î¶¨ Î∂ÄÏ°± Ïãú 64 ÎòêÎäî 32Î°ú ÎÇÆÏ∂îÏÑ∏Ïöî.
BATCH_SIZE = 128
NUM_WORKERS = 16  # Îç∞Ïù¥ÌÑ∞ Î°úÎî© ÏÜçÎèÑ Ìñ•ÏÉÅ

# Î°úÍ∑∏ Î∞è Í≤∞Í≥º ÌååÏùº Í≤ΩÎ°ú (ResNet/EfficientNetÍ≥º Í≤πÏπòÏßÄ ÏïäÍ≤å ÏÑ§Ï†ï)
LOG_FILE = "vit_training_log.txt"
MODEL_SAVE_NAME = "best_finetuned_vit.pth"
REPORT_SAVE_NAME = "vit_classification_report.json"
CM_SAVE_NAME = "vit_confusion_matrix.npy"

# ==========================================
# 2. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨
# ==========================================
# ViTÎäî 224x224 ÏûÖÎ†• Í∂åÏû•
transform_train = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(20),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225],
    ),
])

transform_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225],
    ),
])

# ==========================================
# 3. Îç∞Ïù¥ÌÑ∞ÏÖã Î∞è Îç∞Ïù¥ÌÑ∞Î°úÎçî
# ==========================================
if not os.path.exists(train_dir) or not os.path.exists(test_dir):
    print("Îç∞Ïù¥ÌÑ∞ Í≤ΩÎ°úÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.")
    # exit()

train_dataset = datasets.ImageFolder(root=train_dir, transform=transform_train)
test_dataset = datasets.ImageFolder(root=test_dir, transform=transform_test)

# [Î≥ÄÍ≤Ω] ÏÉÅÌñ•Îêú Batch SizeÏôÄ Num Workers Ï†ÅÏö©
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)

print(f"ÌõàÎ†®ÏÖã ÌÅ¨Í∏∞: {len(train_dataset)}")
print(f"ÌÖåÏä§Ìä∏ÏÖã ÌÅ¨Í∏∞: {len(test_dataset)}")
num_classes = len(train_dataset.classes)
print(f"ÌÅ¥ÎûòÏä§ Ïàò: {num_classes}")

# ==========================================
# 4. Î™®Îç∏ ÏÑ§Ï†ï Î∞è GPU Î≥ëÎ†¨Ìôî
# ==========================================
# ÏÇ¨Ï†Ñ ÌïôÏäµÎêú ViT Î∂àÎü¨Ïò§Í∏∞
model = models.vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)

# Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞ freeze (ÎßàÏßÄÎßâ classification head Ï†úÏô∏)
for param in model.parameters():
    param.requires_grad = False

# ÏµúÏÉÅÏúÑ Î†àÏù¥Ïñ¥ Î≥ÄÍ≤Ω (ÎûúÎç§ Ï¥àÍ∏∞Ìôî)
# torchvisionÏùò ViTÏóêÏÑúÎäî `heads.head`Î°ú Ï†ëÍ∑º
model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)

# ÎßàÏßÄÎßâ Î†àÏù¥Ïñ¥Ïùò ÌååÎùºÎØ∏ÌÑ∞Îßå ÌïôÏäµ Í∞ÄÎä•ÌïòÎèÑÎ°ù ÏÑ§Ï†ï
for param in model.heads.head.parameters():
    param.requires_grad = True

# GPU ÏÑ§Ï†ï
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# [ÌïµÏã¨ Î≥ÄÍ≤Ω] Multi-GPU ÏÑ§Ï†ï (DataParallel)
if torch.cuda.device_count() > 1:
    print(f"üî• [System] Í∞êÏßÄÎêú GPU Í∞úÏàò: {torch.cuda.device_count()}Í∞ú")
    print("üî• DataParallelÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î≥ëÎ†¨ ÌïôÏäµÏùÑ ÏãúÏûëÌï©ÎãàÎã§!")
    model = nn.DataParallel(model)
else:
    print(f"Using single device: {device}")

model = model.to(device)

# ÏÜêÏã§ Ìï®Ïàò
criterion = nn.CrossEntropyLoss()

# ==========================================
# 5. ÌïôÏäµ Î∞è ÌÖåÏä§Ìä∏ Ìï®Ïàò
# ==========================================
def train_and_test(
    model,
    train_loader,
    test_loader,
    criterion,
    num_epochs=30,
    log_file=LOG_FILE,
):
    best_test_acc = 0.0

    # Î°úÍ∑∏ Ìè¥Îçî ÏÉùÏÑ± Î∞è Ìó§Îçî ÏûëÏÑ±
    os.makedirs("./log", exist_ok=True)
    log_path = f"./log/{log_file}"

    with open(log_path, mode='w') as file:
        file.write("Epoch\tTrain Loss\tTrain Acc\tTest Loss\tTest Acc\tTest F1\n")

    print("\n" + "=" * 70)
    print("Vision Transformer (ViT) ÌïôÏäµ ÏãúÏûë")
    print("=" * 70)

    # [Ï§ëÏöî] DataParallel ÏÇ¨Ïö© Ïãú, ÎÇ¥Î∂Ä ÌååÎùºÎØ∏ÌÑ∞(heads.head) Ï†ëÍ∑ºÏùÑ ÏúÑÌï¥ .module ÏÇ¨Ïö©
    real_model = model.module if hasattr(model, 'module') else model

    # Ï¥àÍ∏∞ Optimizer (ClassifierÎßå)
    optimizer = optim.Adam(real_model.heads.head.parameters(), lr=1e-4)

    for epoch in range(num_epochs):
        print(f"\n[Epoch {epoch+1}/{num_epochs}]")

        # ---------------------------------------
        # ÎèôÏ†Å Learning Rate Î∞è Layer Freezing
        # ---------------------------------------
        if epoch == 0:
            print("Ï≤´ Î≤àÏß∏ epoch: ÎßàÏßÄÎßâ Ï∂úÎ†•Ï∏µ(Classifier)Îßå ÌïôÏäµ")
            # Ï†ÑÏ≤¥ ÎèôÍ≤∞
            for param in real_model.parameters(): 
                param.requires_grad = False
            # ClassifierÎßå Ìï¥Ï†ú
            for param in real_model.heads.head.parameters(): 
                param.requires_grad = True
                
            optimizer = optim.Adam(real_model.heads.head.parameters(), lr=1e-4)
            
        elif epoch == 1:
            print("Îëê Î≤àÏß∏ epochÎ∂ÄÌÑ∞ Ï†ÑÏ≤¥ Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ï (Fine-tuning)")
            # Ï†ÑÏ≤¥ Ìï¥Ï†ú
            for param in real_model.parameters(): 
                param.requires_grad = True
                
            optimizer = optim.Adam(real_model.parameters(), lr=1e-5)

        # ---------------------------------------
        # ÌïôÏäµ Îã®Í≥Ñ (Training)
        # ---------------------------------------
        model.train()
        running_loss = 0.0
        running_corrects = 0

        for inputs, labels in tqdm(train_loader, desc="Training"):
            inputs = inputs.to(device)
            labels = labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            _, preds = torch.max(outputs, 1)

            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

        epoch_train_loss = running_loss / len(train_loader.dataset)
        epoch_train_acc = running_corrects.double() / len(train_loader.dataset)
        print(f"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f}")

        # ---------------------------------------
        # ÌèâÍ∞Ä Îã®Í≥Ñ (Testing)
        # ---------------------------------------
        model.eval()
        test_loss = 0.0
        test_corrects = 0
        all_test_preds = []
        all_test_labels = []

        with torch.no_grad():
            for inputs, labels in tqdm(test_loader, desc="Testing"):
                inputs = inputs.to(device)
                labels = labels.to(device)

                outputs = model(inputs)
                loss = criterion(outputs, labels)
                _, preds = torch.max(outputs, 1)

                test_loss += loss.item() * inputs.size(0)
                test_corrects += torch.sum(preds == labels.data)
                
                # CPUÎ°ú Ïù¥ÎèôÌïòÏó¨ Í≤∞Í≥º ÏàòÏßë
                all_test_preds.extend(preds.cpu().numpy())
                all_test_labels.extend(labels.cpu().numpy())

        epoch_test_loss = test_loss / len(test_loader.dataset)
        epoch_test_acc = test_corrects.double() / len(test_loader.dataset)
        epoch_test_f1 = f1_score(all_test_labels, all_test_preds, average='macro')
        
        print(
            f"Test Loss: {epoch_test_loss:.4f} | Test Acc: {epoch_test_acc:.4f} | Test F1: {epoch_test_f1:.4f}"
        )

        # Î°úÍ∑∏ Ï†ÄÏû•
        with open(log_path, mode="a") as file:
            file.write(
                f"{epoch+1}\t{epoch_train_loss:.4f}\t{epoch_train_acc:.4f}\t{epoch_test_loss:.4f}\t{epoch_test_acc:.4f}\t{epoch_test_f1:.4f}\n"
            )

        # ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏ Ï†ÄÏû• (real_model Ï†ÄÏû•)
        if epoch_test_acc > best_test_acc:
            best_test_acc = epoch_test_acc
            torch.save(real_model.state_dict(), MODEL_SAVE_NAME)
            print(f"‚úì Best model saved to {MODEL_SAVE_NAME}")

    print("\n" + "=" * 70)
    print(f"ÌïôÏäµ ÏôÑÎ£å! Best Test Acc: {best_test_acc:.4f}")
    print("=" * 70)

    # ==========================================
    # 6. ÏµúÏ¢Ö Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû•
    # ==========================================
    print("\nÏµúÏ¢Ö ÌèâÍ∞Ä Î∞è Î©îÌä∏Î¶≠ Ï†ÄÏû• Ï§ë...")
    model.eval()
    all_preds = []
    all_labels = []
    all_probs = []

    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc="Final Evaluation"):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            probs = torch.softmax(outputs, dim=1)
            _, preds = torch.max(outputs, 1)

            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    all_probs = np.array(all_probs)

    # 1. Classification Report (JSON)
    class_names = train_loader.dataset.classes
    report = classification_report(
        all_labels, all_preds, target_names=class_names, output_dict=True
    )
    with open(REPORT_SAVE_NAME, "w") as f:
        json.dump(report, f, indent=4)
    print(f"‚úì Classification Report saved to {REPORT_SAVE_NAME}")

    # 2. Confusion Matrix (NPY)
    cm = confusion_matrix(all_labels, all_preds)
    np.save(CM_SAVE_NAME, cm)
    print(f"‚úì Confusion Matrix saved to {CM_SAVE_NAME}")

    # 3. Probabilities (NPY)
    np.save("vit_test_predictions.npy", all_preds)
    np.save("vit_test_labels.npy", all_labels)
    np.save("vit_test_probs.npy", all_probs)
    print("‚úì Test predictions and probabilities saved (vit_ prefix).")

    return model


if __name__ == "__main__":
    model = train_and_test(
        model,
        train_loader,
        test_loader,
        criterion,
        num_epochs=30,
        log_file=LOG_FILE,
    )
    # os.system("shutdown /s /t 60") # ÌïÑÏöî Ïãú Ï£ºÏÑù Ìï¥Ï†ú